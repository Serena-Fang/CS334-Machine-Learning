{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "class Perceptron(object):\n",
    "    mEpoch = 1000  # maximum epoch size\n",
    "    w = None       # weights of the perceptron\n",
    "\n",
    "    def __init__(self, epoch):\n",
    "        self.mEpoch = epoch\n",
    "\n",
    "    def train(self, xFeat, y):\n",
    "        \"\"\"\n",
    "        Train the perceptron using the data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        xFeat : nd-array with shape n x d\n",
    "            Training data \n",
    "        y : 1d array with shape n\n",
    "            Array of responses associated with training data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        stats : object\n",
    "            Keys represent the epochs and values the number of mistakes\n",
    "        \"\"\"\n",
    "        stats = {}\n",
    "        # TODO implement this\n",
    "        self.w = np.zeros(1+xFeat.shape[1])\n",
    "\n",
    "        for epoch in range(self.mEpoch):\n",
    "            mistakes = 0\n",
    "            for i in range(len(xFeat)):\n",
    "                delta_w = 0.01 * (y[i] - self.predict(xFeat[i]))\n",
    "                self.w[1:] += delta_w * xFeat[i]\n",
    "                self.w[0] += delta_w\n",
    "                mistakes += int(delta_w != 0.0)\n",
    "            if mistakes == 0:\n",
    "                break\n",
    "            stats[epoch] = mistakes\n",
    "        return stats\n",
    "\n",
    "    def predict(self, xFeat):\n",
    "        \"\"\"\n",
    "        Given the feature set xFeat, predict \n",
    "        what class the values will have.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        xFeat : nd-array with shape m x d\n",
    "            The data to predict.  \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        yHat : 1d array or list with shape m\n",
    "            Predicted response per sample\n",
    "        \"\"\"\n",
    "        yHat = []\n",
    "        yHat = np.dot(xFeat, self.w[1:]) + self.w[0]\n",
    "        yHat = np.where(yHat >= 0.0, 1, 0)\n",
    "        return yHat\n",
    "\n",
    "    def pos_neg_words(self, data):\n",
    "        weights = self.w\n",
    "        words = list(data.columns.values)\n",
    "        np_words = np.array(words)\n",
    "        \n",
    "        positive_index = np.argsort(-weights)[:15]\n",
    "        pos_list = np_words[positive_index]\n",
    "        negative_index = np.argsort(weights)[:15]\n",
    "        neg_list = np_words[negative_index]\n",
    "        return pos_list, neg_list\n",
    "\n",
    "def calc_mistakes(yHat, yTrue):\n",
    "    \"\"\"\n",
    "    Calculate the number of mistakes\n",
    "    that the algorithm makes based on the prediction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yHat : 1-d array or list with shape n\n",
    "        The predicted label.\n",
    "    yTrue : 1-d array or list with shape n\n",
    "        The true label.      \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    err : int\n",
    "        The number of mistakes that are made\n",
    "    \"\"\"\n",
    "    mistake = 0\n",
    "    for i in range(len(yTrue)):\n",
    "        if yHat[i] != yTrue[i]:\n",
    "            mistake += 1\n",
    "    return mistake\n",
    "\n",
    "\n",
    "def file_to_numpy(filename):\n",
    "    \"\"\"\n",
    "    Read an input file and convert it to numpy\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    return df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from perceptron import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_train = file_to_numpy('binary_train.csv')\n",
    "b_test = file_to_numpy('binary_test.csv')\n",
    "c_train = file_to_numpy('count_train.csv')\n",
    "c_test = file_to_numpy('count_test.csv')\n",
    "y_train = file_to_numpy('y_train.csv')\n",
    "y_test = file_to_numpy('y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mistakes(yHat, yTrue):\n",
    "    mistake = 0\n",
    "    for i in range(len(yTrue)):\n",
    "        if yHat[i] != yTrue[i]:\n",
    "            mistake += 1\n",
    "    return mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_epoch(x, y, mEpoch_list):\n",
    "    epoch_mistake = {}\n",
    "    k = len(mEpoch_list)\n",
    "    kf = KFold(n_splits=k)\n",
    "    for mEpoch in mEpoch_list:\n",
    "        mistakes_list = []\n",
    "        for train_i, test_i in kf.split(x):\n",
    "            xTrain, xTest = x[train_i], x[test_i]\n",
    "            yTrain, yTest = y[train_i], y[test_i]\n",
    "            p = Perceptron(mEpoch)\n",
    "            p.train(xTrain, yTrain)\n",
    "            yHat = p.predict(xTest)\n",
    "            mistakes = calc_mistakes(yHat, yTest)\n",
    "            mistakes_list.append(mistakes)\n",
    "        avg_mistake = np.mean(mistakes_list)\n",
    "        epoch_mistake[mEpoch] = avg_mistake\n",
    "    print(epoch_mistake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5: 20.666666666666668, 10: 14.5, 15: 14.166666666666666, 20: 13.833333333333334, 30: 13.833333333333334, 60: 13.833333333333334}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mEpoch_list = [5, 10, 15, 20, 30, 60]\n",
    "b_epoch = kfold_epoch(b_train, y_train, mEpoch_list)\n",
    "type(b_epoch)\n",
    "# b_best_epoch = min(b_epoch, key=b_epoch.get)\n",
    "# print(b_epoch)\n",
    "# print(b_best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5: 34.5, 10: 137.125, 15: 30.875, 20: 21.125, 30: 20.75, 60: 16.625, 100: 14.0, 200: 14.0}\n"
     ]
    }
   ],
   "source": [
    "print(kfold_epoch(c_train, y_train, mEpoch_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mistakes on the binary test dataset:\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "# 15 is the optimal number of epochs for binary dataset\n",
    "b_model = Perceptron(15)\n",
    "b_model.train(b_train, y_train)\n",
    "yHat = b_model.predict(b_test)\n",
    "# print out the number of mistakes\n",
    "print(\"Number of mistakes on the binary test dataset:\")\n",
    "print(calc_mistakes(yHat, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mistakes on the count test dataset:\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "# 100 is the optimal number of epochs for binary dataset\n",
    "c_model = Perceptron(100)\n",
    "c_model.train(c_train, y_train)\n",
    "yHat = c_model.predict(c_test)\n",
    "# print out the number of mistakes\n",
    "print(\"Number of mistakes on the count test dataset:\")\n",
    "print(calc_mistakes(yHat, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_neg_words(data):\n",
    "    weights = self.w\n",
    "    words = list(data.columns.values)\n",
    "    np_words = np.array(words)\n",
    "    positive_index = np.argsort(-weights)[:15]\n",
    "    pos_list = np_words[positive_index]\n",
    "    negative_index = np.argsort(weights)[:15]\n",
    "    neg_list = np_words[negative_index]\n",
    "    return pos_list, neg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary dataset: 15 words with most positive weights: \n",
      "['renam' 'guess' 'client' 'martin' 'sign' 'amaz' 'pleasur' 'advanc'\n",
      " 'mirror' 'freedom' 'william' 'thi' 'mail' 'offic' 'oppos']\n",
      "Binary dataset: 15 words with most negative weights: \n",
      "['www' 'us' 'rss' 'core' 'premium' 'item' 'channel' 'settl' 'annual'\n",
      " 'never' 'onc' 'servic' 'spamassassin' 'resid' 'sport']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('binary_train.csv')\n",
    "pos_list, neg_list = b_model.pos_neg_words(df)\n",
    "print(\"Binary dataset: 15 words with most positive weights: \")\n",
    "print(pos_list)\n",
    "print(\"Binary dataset: 15 words with most negative weights: \")\n",
    "print(neg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count dataset: 15 words with most positive weights: \n",
      "['renam' 'monitor' 'org' 'nation' 'isn' 'blame' 'numberbit' 'martin'\n",
      " 'repositori' 'season' 'manner' 'guess' 'sender' 'compar' 'monthli']\n",
      "Count dataset: 15 words with most negative weights: \n",
      "['button' 'spamassassin' 'met' 'www' 'dave' 'newslett' 'filenam' 'us'\n",
      " 'cnumber' 'reach' 'servic' 'the' 'numberpm' 'settl' 'hand']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('count_train.csv')\n",
    "pos_list, neg_list = c_model.pos_neg_words(df)\n",
    "print(\"Count dataset: 15 words with most positive weights: \")\n",
    "print(pos_list)\n",
    "print(\"Count dataset: 15 words with most negative weights: \")\n",
    "print(neg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(\"xTrain_count.csv\").to_numpy()\n",
    "y = pd.read_csv(\"yTrain.csv\").to_numpy()\n",
    "\n",
    "k = 6\n",
    "kf = KFold(n_splits=k)\n",
    "mEpoch_list = [5, 10, 20, 50, 70, 100]\n",
    "\n",
    "k_epoch = {}\n",
    "\n",
    "for mEpoch in mEpoch_list:\n",
    "    mistakes = []\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        xTrain, xTest = x[train_index], x[test_index]\n",
    "        yTrain, yTest = y[train_index], y[test_index]\n",
    "        # in each split, create a new perceptron\n",
    "        perceptron = Perceptron(mEpoch)\n",
    "        # train\n",
    "        perceptron.train(xTrain, yTrain)\n",
    "        # predict\n",
    "        yHat = perceptron.predict(xTest)\n",
    "        # calculate mistakes\n",
    "        mistakes = calc_mistakes(yHat, yTest)\n",
    "        mistakes.append(mistakes)\n",
    "    # average mistake of the cross validation for this mEpoch value\n",
    "    avg_mistake = np.mean(mistakes)\n",
    "    # append the performance to the dict\n",
    "    dict_performance[mEpoch] = avg_mistake\n",
    "\n",
    "print(dict_performance)\n",
    "# mEpoch = 70 would be the best choice.\n",
    "perceptron = Perceptron(70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
